{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import multiprocessing\n",
    "import uuid\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification, pipeline\n",
    ")\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from seqeval.metrics import classification_report\n",
    "\n",
    "# ✅ CONFIGURATION (SET YOUR PATHS)\n",
    "input_root = r\"C:\\Users\\subha\\Box\\Input\"\n",
    "output_root = r\"C:\\Users\\subha\\Box\\Output_Deidentified_Notes\"\n",
    "scrubber_exe = r\"C:\\Users\\subha\\Downloads\\scrubber\\scrubber.19.0411W\\scrubber.19.0411W.exe\"\n",
    "\n",
    "cleaned_notes_folder = r\"C:\\Users\\subha\\Box\\Pipeline\\Cleaned_Notes\"\n",
    "sdoh_output_csv = r\"C:\\Users\\subha\\Box\\Pipeline\\sdoh_aria_results.csv\"\n",
    "bio_output_csv = r\"C:\\Users\\subha\\OneDrive\\Desktop\\sdoh_bio_dataset_corrected.csv\"\n",
    "model_save_path = r\"C:\\Users\\subha\\Documents\\MyTrainedModels\\biobert_ner_trained\"\n",
    "\n",
    "model_checkpoint = \"dmis-lab/biobert-base-cased-v1.1\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ✅ Load NER Model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)\n",
    "model.to(device)\n",
    "\n",
    "# ✅ STEP 1: RUN NLM SCRUBBER FOR DE-IDENTIFICATION\n",
    "# (Code remains unchanged)\n",
    "\n",
    "# ✅ STEP 2: CLEAN NOTES (REMOVE METADATA)\n",
    "# (Code remains unchanged)\n",
    "\n",
    "# ✅ STEP 3: EXTRACT SDOH ENTITIES\n",
    "# (Code remains unchanged)\n",
    "\n",
    "# ✅ STEP 4: CONVERT TO BIO FORMAT\n",
    "\n",
    "def clean_token(token):\n",
    "    \"\"\"Removes unnecessary symbols and keeps only meaningful words.\"\"\"\n",
    "    return re.sub(r\"[^a-zA-Z0-9,.!?-]\", \"\", token)\n",
    "\n",
    "def convert_to_bio_csv(df, output_csv_path):\n",
    "    \"\"\"Converts extracted SDOH dataset into BIO format with proper Sentence_ID tracking.\"\"\"\n",
    "    bio_data = []\n",
    "    sentence_id = 0\n",
    "    last_text = None\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        text = str(row[\"Extracted_Text\"]).strip()\n",
    "        category = str(row[\"SDOH_Category\"]).strip()\n",
    "\n",
    "        if pd.isna(text) or pd.isna(category) or text == \"\":\n",
    "            continue  # Skip empty text\n",
    "\n",
    "        # Increment sentence_id only when text changes\n",
    "        if text != last_text:\n",
    "            sentence_id += 1\n",
    "            last_text = text\n",
    "\n",
    "        tokens = text.split()\n",
    "        tokens = [clean_token(token) for token in tokens if token.strip()]\n",
    "\n",
    "        if len(tokens) == 0:\n",
    "            continue  # Skip if no valid tokens remain\n",
    "\n",
    "        bio_tags = [\"O\"] * len(tokens)  # Default to \"O\"\n",
    "        bio_tags[0] = f\"B-{category}\"\n",
    "        for i in range(1, len(tokens)):\n",
    "            bio_tags[i] = f\"I-{category}\"\n",
    "\n",
    "        for token, tag in zip(tokens, bio_tags):\n",
    "            bio_data.append([sentence_id, token, tag])\n",
    "\n",
    "    bio_df = pd.DataFrame(bio_data, columns=[\"Sentence_ID\", \"Token\", \"BIO_Tag\"])\n",
    "    bio_df.to_csv(output_csv_path, index=False, encoding=\"utf-8\")\n",
    "    print(f\"✅ BIO dataset saved as CSV at: {output_csv_path}\")\n",
    "\n",
    "# ✅ Convert dataset to BIO format and save as CSV\n",
    "df = pd.read_csv(sdoh_output_csv)\n",
    "convert_to_bio_csv(df, bio_output_csv)\n",
    "\n",
    "# ✅ Tokenization Function\n",
    "def tokenize_and_align_labels(texts, labels):\n",
    "    \"\"\"Tokenizes and aligns BIO labels for Named Entity Recognition (NER) model training.\"\"\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "        is_split_into_words=True,\n",
    "    )\n",
    "\n",
    "    aligned_labels = []\n",
    "    for i, label in enumerate(labels):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to words\n",
    "        previous_word = None\n",
    "        new_labels = []\n",
    "\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                new_labels.append(-100)  # Ignore padding\n",
    "            elif word_idx != previous_word:\n",
    "                new_labels.append(label[word_idx])  # Assign correct label\n",
    "            else:\n",
    "                new_labels.append(label[word_idx])  # Keep same label for subwords\n",
    "            previous_word = word_idx\n",
    "\n",
    "        aligned_labels.append(new_labels)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = aligned_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# ✅ STEP 5: TRAIN BIOBERT\n",
    "def train_biobert():\n",
    "    \"\"\"Trains a BioBERT model on the BIO-formatted dataset.\"\"\"\n",
    "    df = pd.read_csv(bio_output_csv)\n",
    "    sentences = df.groupby(\"Sentence_ID\")[\"Token\"].apply(list).tolist()\n",
    "    labels = df.groupby(\"Sentence_ID\")[\"BIO_Tag\"].apply(list).tolist()\n",
    "    label2id = {label: i for i, label in enumerate(sorted(set(df[\"BIO_Tag\"])))}\n",
    "    label_ids = [[label2id[l] for l in lbl] for lbl in labels]\n",
    "    train_texts, val_texts, train_labels, val_labels = train_test_split(sentences, label_ids, test_size=0.2)\n",
    "\n",
    "    # ✅ Tokenize Data\n",
    "    train_data = tokenize_and_align_labels(train_texts, train_labels)\n",
    "    val_data = tokenize_and_align_labels(val_texts, val_labels)\n",
    "\n",
    "    train_dataset = Dataset.from_dict(train_data)\n",
    "    val_dataset = Dataset.from_dict(val_data)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label2id))\n",
    "    training_args = TrainingArguments(output_dir=\"./biobert_ner\", evaluation_strategy=\"epoch\", save_strategy=\"epoch\", per_device_train_batch_size=16, per_device_eval_batch_size=16, num_train_epochs=5, weight_decay=0.01)\n",
    "    trainer = Trainer(model=model, args=training_args, train_dataset=train_dataset, eval_dataset=val_dataset)\n",
    "    trainer.train()\n",
    "    trainer.save_model(model_save_path)\n",
    "    print(f\"✅ Model saved at {model_save_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
