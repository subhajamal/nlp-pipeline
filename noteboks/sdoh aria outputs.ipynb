{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at michiyasunaga/BioLinkBERT-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "# ✅ Use the Best SDOH Model\n",
    "model_name = \"michiyasunaga/BioLinkBERT-large\"\n",
    "\n",
    "# ✅ Load tokenizer & model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "# ✅ Enable GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# ✅ Load NER pipeline\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\", device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "print(\"✅ Model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1067 SDOH keywords successfully!\n",
      "🔹 Sample Keywords: ['laid', 'attorney', 'dad', 'use/social', '2-1/2', 'office', '18', 'imposed', 'alcohol-containing', 'outside']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# ✅ Load SDOH keywords\n",
    "keywords_path = r\"C:\\Users\\subha\\Downloads\\keywords.json\"  # \n",
    "\n",
    "with open(keywords_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    sdoh_keywords = set(json.load(f).keys())\n",
    "\n",
    "print(f\"✅ Loaded {len(sdoh_keywords)} SDOH keywords successfully!\")\n",
    "print(\"🔹 Sample Keywords:\", list(sdoh_keywords)[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SDOH Categories Loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\subha/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.download(\"punkt\")  # Required for sentence tokenization\n",
    "\n",
    "# ✅ Define SDOH categories\n",
    "sdoh_mapping = {\n",
    "    \"SMOKING\": {\"smoking\", \"cigarettes\", \"tobacco\", \"vaping\", \"e-cigarettes\", \"nicotine\"},\n",
    "    \"DRUG_USE\": {\"cocaine\", \"heroin\", \"substance use\", \"opioids\", \"marijuana\", \"methamphetamine\", \"prescription drugs\"},\n",
    "    \"ALCOHOL_USE\": {\"alcohol\", \"binge drinking\", \"drinking\", \"wine\", \"beer\", \"spirits\", \"liquor\"},\n",
    "    \"HOUSING\": {\"homeless\", \"unstable housing\", \"shelter\", \"housing insecurity\", \"eviction\", \"foreclosure\"},\n",
    "    \"SOCIAL_SUPPORT\": {\"lives alone\", \"no family\", \"social isolation\", \"social support\", \"community\", \"family\"},\n",
    "    \"EMPLOYMENT\": {\"unemployed\", \"job loss\", \"joblessness\", \"income\", \"underemployment\", \"workplace stress\"},\n",
    "    \"TRANSPORTATION\": {\"no transportation\", \"lack of transportation\", \"public transport\", \"vehicle access\", \"transportation barrier\"},\n",
    "    \"MENTAL_HEALTH\": {\"depression\", \"anxiety\", \"PTSD\", \"mental illness\", \"stress\", \"bipolar\", \"schizophrenia\", \"mood disorder\"},\n",
    "    \"EDUCATION\": {\"low education\", \"no high school\", \"GED\", \"college\", \"education level\", \"graduation\", \"dropout\"},\n",
    "    \"FOOD_SECURITY\": {\"food insecurity\", \"hunger\", \"food access\", \"nutrition\", \"food desert\", \"malnutrition\", \"grocery store access\"},\n",
    "    \"HEALTHCARE_ACCESS\": {\"lack of healthcare\", \"health insurance\", \"uninsured\", \"underinsured\", \"medical care\", \"healthcare access\"},\n",
    "    \"SAFETY\": {\"violence\", \"domestic abuse\", \"bullying\", \"crime\", \"unsafe neighborhood\", \"gun violence\", \"child abuse\", \"elder abuse\"},\n",
    "}\n",
    "\n",
    "print(\"✅ SDOH Categories Loaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_sdoh_category(entity_text):\n",
    "    entity_text_lower = entity_text.lower()\n",
    "\n",
    "    # ✅ First, check against predefined SDOH keywords\n",
    "    if entity_text_lower in sdoh_keywords:\n",
    "        return sdoh_keywords[entity_text_lower]  # Get the correct category\n",
    "\n",
    "    # ✅ If not found in predefined keywords, use existing SDOH mapping\n",
    "    for category, keywords in sdoh_mapping.items():\n",
    "        if any(word in entity_text_lower for word in keywords):\n",
    "            return category\n",
    "\n",
    "    return \"OTHER\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SDOH Mapping Function Ready!\n",
      "✅ SDOH Extraction Pipeline Ready!\n"
     ]
    }
   ],
   "source": [
    "# ✅ Function to Map Extracted Entities to SDOH Categories\n",
    "def map_to_sdoh_category(entity_text):\n",
    "    entity_text_lower = entity_text.lower()\n",
    "    for category, keywords in sdoh_mapping.items():\n",
    "        if any(word in entity_text_lower for word in keywords):\n",
    "            return category\n",
    "    return \"OTHER\"\n",
    "\n",
    "print(\"✅ SDOH Mapping Function Ready!\")\n",
    "\n",
    "# ✅ Smart Preprocessing: Extract Key Sections (If Available)\n",
    "def preprocess_note(text):\n",
    "    \"\"\"\n",
    "    Extracts important sections of the note (SOCIAL HISTORY, ASSESSMENT, etc.).\n",
    "    Falls back to full text if no structure is detected.\n",
    "    \"\"\"\n",
    "    sections = []\n",
    "    pattern = re.compile(r\"(SOCIAL HISTORY|HISTORY OF PRESENT ILLNESS|ASSESSMENT|PLAN|REVIEW OF SYSTEMS):(.+?)(?=\\n[A-Z\\s]+:|\\Z)\", re.DOTALL)\n",
    "    matches = pattern.findall(text)\n",
    "    \n",
    "    for match in matches:\n",
    "        sections.append(match[1].strip())\n",
    "\n",
    "    return \"\\n\".join(sections) if sections else text  # Return extracted sections OR full text\n",
    "\n",
    "# ✅ Sentence-Based Chunking for NER Processing\n",
    "def extract_sdoh_from_text(text):\n",
    "    text = preprocess_note(text)  # ✅ Extract key sections first\n",
    "    sentences = nltk.sent_tokenize(text)  # ✅ Split into sentences\n",
    "    sdoh_entities = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        input_ids = tokenizer.encode(sentence, add_special_tokens=True)\n",
    "\n",
    "        # ✅ Skip empty or too short sentences\n",
    "        if len(input_ids) == 0:\n",
    "            continue\n",
    "\n",
    "        # ✅ Process long sentences in safe token-based chunks\n",
    "        if len(input_ids) > 512:\n",
    "            for i in range(0, len(input_ids), 450):  # ✅ Overlapping chunks\n",
    "                chunk_ids = input_ids[i:i + 512]\n",
    "                chunk_text = tokenizer.decode(chunk_ids, skip_special_tokens=True)\n",
    "                chunk_entities = process_chunk(chunk_text)\n",
    "                sdoh_entities.extend(chunk_entities)\n",
    "        else:\n",
    "            chunk_entities = process_chunk(sentence)\n",
    "            sdoh_entities.extend(chunk_entities)\n",
    "\n",
    "    return sdoh_entities\n",
    "\n",
    "# ✅ Function to Process a Single Chunk (Fixed for 512 Token Limit)\n",
    "def process_chunk(chunk_text):\n",
    "    \"\"\"Runs the NER pipeline on a text chunk and returns extracted entities.\"\"\"\n",
    "    chunk_entities = []\n",
    "    \n",
    "    # ✅ Ensure input does not exceed 512 tokens\n",
    "    input_ids = tokenizer.encode(chunk_text, add_special_tokens=True, truncation=True, max_length=512)\n",
    "\n",
    "    chunk_text = tokenizer.decode(input_ids, skip_special_tokens=True)  # ✅ Convert back to text\n",
    "\n",
    "    # ✅ Run NER model safely\n",
    "    ner_results = ner_pipeline(chunk_text)\n",
    "    for entity in ner_results:\n",
    "        entity_text = entity[\"word\"]\n",
    "        entity_type = map_to_sdoh_category(entity_text)\n",
    "        if entity_type != \"OTHER\":\n",
    "            chunk_entities.append((entity_text, entity_type))\n",
    "\n",
    "    return chunk_entities\n",
    "\n",
    "print(\"✅ SDOH Extraction Pipeline Ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Extracted SDOH Entities:\n",
      "🔹 Word: has a history of smoking and | SDOH Category: SMOKING\n",
      "🔹 Word: reports alcohol use. | SDOH Category: ALCOHOL_USE\n",
      "🔹 Word: no transportation. | SDOH Category: TRANSPORTATION\n",
      "🔹 Word: there are signs of anxiety and depression, and he lives | SDOH Category: MENTAL_HEALTH\n",
      "🔹 Word: alone in unstable housing. | SDOH Category: HOUSING\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# ✅ Sample Test (Before Running on 6,000+ Notes)\n",
    "sample_text = \"\"\"\n",
    "The patient has a history of smoking and reports alcohol use.\n",
    "He has been unemployed for the last 6 months and has no transportation.\n",
    "There are signs of anxiety and depression, and he lives alone in unstable housing.\n",
    "\"\"\"\n",
    "\n",
    "# ✅ Extract & Display SDOH\n",
    "sdoh_results = extract_sdoh_from_text(sample_text)\n",
    "print(\"\\n✅ Extracted SDOH Entities:\")\n",
    "for entity, category in sdoh_results:\n",
    "    print(f\"🔹 Word: {entity} | SDOH Category: {category}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Extracted SDOH Entities:\n",
      "🔹 Word: has a history of smoking and | SDOH Category: SMOKING\n",
      "🔹 Word: reports alcohol use. | SDOH Category: ALCOHOL_USE\n",
      "🔹 Word: no transportation. | SDOH Category: TRANSPORTATION\n",
      "🔹 Word: there are signs of anxiety and depression, and he lives | SDOH Category: MENTAL_HEALTH\n",
      "🔹 Word: alone in unstable housing. | SDOH Category: HOUSING\n"
     ]
    }
   ],
   "source": [
    "# ✅ Sample clinical note\n",
    "sample_text = \"\"\"\n",
    "The patient has a history of smoking and reports alcohol use.\n",
    "He has been unemployed for the last 6 months and has no transportation.\n",
    "There are signs of anxiety and depression, and he lives alone in unstable housing.\n",
    "\"\"\"\n",
    "\n",
    "# ✅ Extract SDOH entities\n",
    "sdoh_results = extract_sdoh_from_text(sample_text)\n",
    "\n",
    "# ✅ Display results\n",
    "print(\"\\n✅ Extracted SDOH Entities:\")\n",
    "for entity, category in sdoh_results:\n",
    "    print(f\"🔹 Word: {entity} | SDOH Category: {category}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found 6435 files to process in C:\\Users\\subha\\Box\\Pipeline\\Cleaned_Notes\\aria\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Notes:   2%|▏         | 101/6435 [10:20<9:22:22,  5.33s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved progress at 100 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Notes:   3%|▎         | 201/6435 [17:17<6:14:41,  3.61s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved progress at 200 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Notes:   5%|▍         | 301/6435 [35:29<37:33:46, 22.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved progress at 300 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Notes:   6%|▌         | 401/6435 [53:24<14:15:26,  8.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved progress at 400 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Notes:   8%|▊         | 501/6435 [1:09:29<22:48:12, 13.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved progress at 500 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Notes:   9%|▉         | 601/6435 [1:22:56<18:15:28, 11.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved progress at 600 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Notes:  11%|█         | 701/6435 [1:45:40<49:10:25, 30.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved progress at 700 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Notes:  11%|█         | 713/6435 [1:50:00<30:05:17, 18.93s/it]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ✅ Input Folder\n",
    "input_folder = r\"C:\\Users\\subha\\Box\\Pipeline\\Cleaned_Notes\\aria\"\n",
    "\n",
    "# ✅ Output File Path\n",
    "output_path = r\"C:\\Users\\subha\\Box\\Pipeline\\sdoh_aria_results.csv\"\n",
    "\n",
    "# ✅ Recursively find all .txt files in subdirectories\n",
    "all_files = []\n",
    "for root, _, files in os.walk(input_folder):\n",
    "    for file in files:\n",
    "        if file.endswith(\".txt\"):\n",
    "            all_files.append(os.path.join(root, file))  # Save full file path\n",
    "\n",
    "print(f\"✅ Found {len(all_files)} files to process in {input_folder}\")\n",
    "\n",
    "# ✅ Load existing results (to resume if script restarts)\n",
    "if os.path.exists(output_path):\n",
    "    existing_df = pd.read_csv(output_path)\n",
    "    processed_files = set(existing_df[\"File\"].unique())  # Track already processed files\n",
    "    print(f\"🔄 Resuming from {len(processed_files)} processed files...\")\n",
    "else:\n",
    "    processed_files = set()\n",
    "    existing_df = pd.DataFrame(columns=[\"File\", \"Extracted_Text\", \"SDOH_Category\"])\n",
    "\n",
    "# ✅ Process All Notes\n",
    "sdoh_results = []\n",
    "\n",
    "with tqdm(total=len(all_files), desc=\"Processing Notes\") as pbar:\n",
    "    for idx, file_path in enumerate(all_files):\n",
    "        file_name = os.path.basename(file_path)  # Get just the file name\n",
    "        \n",
    "        # ✅ Skip if already processed\n",
    "        if file_name in processed_files:\n",
    "            pbar.update(1)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                note_text = f.read().strip()\n",
    "\n",
    "            if not note_text:\n",
    "                continue\n",
    "\n",
    "            # ✅ Extract SDOH\n",
    "            extracted_sdoh = extract_sdoh_from_text(note_text)\n",
    "\n",
    "            for entity_text, entity_type in extracted_sdoh:\n",
    "                sdoh_results.append([file_name, entity_text, entity_type])\n",
    "\n",
    "            # ✅ Save progress every 100 files\n",
    "            if idx % 100 == 0 and idx > 0:\n",
    "                df_temp = pd.DataFrame(sdoh_results, columns=[\"File\", \"Extracted_Text\", \"SDOH_Category\"])\n",
    "                df_temp.to_csv(output_path, mode=\"a\", header=not os.path.exists(output_path), index=False)  # Append mode\n",
    "                print(f\"✅ Saved progress at {idx} files\")\n",
    "                sdoh_results = []  # Reset list after saving\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error in {file_path}: {e}\")\n",
    "\n",
    "        pbar.update(1)\n",
    "\n",
    "# ✅ Final Save\n",
    "if sdoh_results:\n",
    "    df_temp = pd.DataFrame(sdoh_results, columns=[\"File\", \"Extracted_Text\", \"SDOH_Category\"])\n",
    "    df_temp.to_csv(output_path, mode=\"a\", header=not os.path.exists(output_path), index=False)\n",
    "\n",
    "print(f\"✅ Final results saved at: {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
